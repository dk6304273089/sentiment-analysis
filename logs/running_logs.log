[2022-03-16 22:26:47,197: INFO: stage_02_preprocessing]: 
********************
[2022-03-16 22:26:47,197: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Preprocessing Data started <<<<<
[2022-03-16 22:26:47,197: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-16 22:26:47,197: INFO: common]: created directory at: data\processed_data
[2022-03-16 22:26:47,213: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection started
[2022-03-16 22:27:14,120: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection Ended
[2022-03-16 22:27:14,120: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Started
[2022-03-16 22:27:14,620: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Ended
[2022-03-16 22:27:14,620: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Started
[2022-03-16 22:27:15,277: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Ended
[2022-03-16 22:27:15,277: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Started
[2022-03-16 22:27:21,679: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Ended
[2022-03-16 22:27:21,726: INFO: stage_02_preprocessing]: Completed all the Stages in preprocessing data was stored in data\processed_data\processed.csv
[2022-03-16 22:27:21,726: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Preprocessing Data completed!<<<<<

[2022-03-16 22:27:26,010: INFO: stage_01_get_data]: 
********************
[2022-03-16 22:27:26,010: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data started <<<<<
[2022-03-16 22:27:26,010: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-16 22:27:26,010: INFO: common]: yaml file: configs/credentials.yaml loaded successfully
[2022-03-16 22:27:26,010: INFO: common]: created directory at: data
[2022-03-16 22:27:26,010: INFO: common]: created directory at: data\extracted_data
[2022-03-16 22:27:26,010: INFO: stage_01_get_data]: verifying the credentials
[2022-03-16 22:27:47,337: INFO: stage_01_get_data]: Started downloading the data
[2022-03-16 22:27:47,549: INFO: stage_01_get_data]: Successfully saved the data in data\extracted_data\products.csv
[2022-03-16 22:27:48,016: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data completed!<<<<<

[2022-03-17 11:12:50,483: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-17 11:13:37,460: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-17 11:15:10,245: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-17 20:08:55,752: INFO: stage_02_preprocessing]: 
********************
[2022-03-17 20:08:55,774: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Preprocessing Data started <<<<<
[2022-03-17 20:08:55,778: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-17 20:08:55,778: INFO: common]: created directory at: data\processed_data
[2022-03-17 20:08:56,093: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection started
[2022-03-17 20:09:22,078: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection Ended
[2022-03-17 20:09:22,078: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Started
[2022-03-17 20:09:22,884: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Ended
[2022-03-17 20:09:22,884: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Started
[2022-03-17 20:09:23,539: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Ended
[2022-03-17 20:09:23,539: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Started
[2022-03-17 20:09:24,603: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Ended
[2022-03-17 20:09:24,964: INFO: stage_02_preprocessing]: Completed all the Stages in preprocessing data was stored in data\processed_data\processed.csv
[2022-03-17 20:09:24,964: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Preprocessing Data completed!<<<<<

[2022-03-17 20:09:57,655: INFO: stage_03_feature_engineering]: 
********************
[2022-03-17 20:09:57,655: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering started <<<<<
[2022-03-17 20:09:57,660: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-17 20:09:57,661: INFO: common]: created directory at: data\feature_processed
[2022-03-17 20:09:57,669: INFO: stage_03_feature_engineering]: Added New Columns {Noun Strength,Review Polarity,Review Subjectivity,Review Complexity,Service Tagger,Compound Score}
[2022-03-17 20:11:39,594: INFO: stage_03_feature_engineering]: Successfully saved the file in data\feature_processed\feature.csv
[2022-03-17 20:11:39,594: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering completed!<<<<<

[2022-03-17 20:11:47,258: INFO: stage_01_get_data]: 
********************
[2022-03-17 20:11:47,259: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data started <<<<<
[2022-03-17 20:11:47,262: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-17 20:11:47,264: INFO: common]: yaml file: configs/credentials.yaml loaded successfully
[2022-03-17 20:11:47,264: INFO: common]: created directory at: data
[2022-03-17 20:11:47,264: INFO: common]: created directory at: data\extracted_data
[2022-03-17 20:11:47,265: INFO: stage_01_get_data]: verifying the credentials
[2022-03-17 20:12:59,717: ERROR: stage_01_get_data]: An error occurred (NoSuchBucket) when calling the GetObject operation: The specified bucket does not exist
Traceback (most recent call last):
  File "src/stage_01_get_data.py", line 43, in main
    csv_obj = client.get_object(Bucket=bucketname, Key=filename)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\client.py", line 395, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\client.py", line 725, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.NoSuchBucket: An error occurred (NoSuchBucket) when calling the GetObject operation: The specified bucket does not exist
[2022-03-17 20:12:59,781: ERROR: stage_01_get_data]: An error occurred (NoSuchBucket) when calling the GetObject operation: The specified bucket does not exist
Traceback (most recent call last):
  File "src/stage_01_get_data.py", line 62, in <module>
    main(config_path=parsed_args.config,creditionals_path=parsed_args.credentials)
  File "src/stage_01_get_data.py", line 52, in main
    raise e
  File "src/stage_01_get_data.py", line 43, in main
    csv_obj = client.get_object(Bucket=bucketname, Key=filename)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\client.py", line 395, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\client.py", line 725, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.NoSuchBucket: An error occurred (NoSuchBucket) when calling the GetObject operation: The specified bucket does not exist
[2022-03-27 12:36:12,722: INFO: stage_01_get_data]: 
********************
[2022-03-27 12:36:12,769: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data started <<<<<
[2022-03-27 12:36:12,785: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-27 12:36:12,785: INFO: common]: yaml file: configs/credentials.yaml loaded successfully
[2022-03-27 12:36:12,785: INFO: common]: created directory at: data
[2022-03-27 12:36:12,785: INFO: common]: created directory at: data\extracted_data
[2022-03-27 12:36:12,785: INFO: stage_01_get_data]: verifying the credentials
[2022-03-27 12:36:26,749: INFO: stage_01_get_data]: Started downloading the data
[2022-03-27 12:36:30,296: INFO: stage_01_get_data]: Successfully saved the data in data\extracted_data\products.csv
[2022-03-27 12:36:30,390: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data completed!<<<<<

[2022-03-27 12:39:33,869: INFO: stage_03_feature_engineering]: 
********************
[2022-03-27 12:39:33,869: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering started <<<<<
[2022-03-27 12:39:33,869: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-27 12:39:33,869: INFO: common]: created directory at: data\feature_processed
[2022-03-27 12:39:33,916: INFO: stage_03_feature_engineering]:  Added New Columns {Noun Strength,Review Polarity,Review Subjectivity,Review Complexity,Service Tagger,Compound Score}
[2022-03-27 12:39:50,830: ERROR: stage_03_feature_engineering]: 
**********************************************************************
  Resource [93mwordnet[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('wordnet')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/wordnet[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************
Traceback (most recent call last):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 84, in __load
    root = nltk.data.find(f"{self.subdir}/{zip_name}")
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93mwordnet[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('wordnet')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/wordnet.zip/wordnet/[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/stage_03_feature_engineering.py", line 165, in main
    df.loc[df['product'] == product, 'Rn'] = self.noun_score(data['answer_option'].values).values
  File "src/stage_03_feature_engineering.py", line 102, in noun_score
    noun_tag.append([lemmatizer.lemmatize(token.lemma_) for token in doc if
  File "src/stage_03_feature_engineering.py", line 102, in <listcomp>
    noun_tag.append([lemmatizer.lemmatize(token.lemma_) for token in doc if
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\stem\wordnet.py", line 45, in lemmatize
    lemmas = wn._morphy(word, pos)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 121, in __getattr__
    self.__load()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 86, in __load
    raise e
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 81, in __load
    root = nltk.data.find(f"{self.subdir}/{self.__name}")
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93mwordnet[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('wordnet')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/wordnet[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************

[2022-03-27 12:39:50,986: ERROR: stage_03_feature_engineering]: 
**********************************************************************
  Resource [93mwordnet[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('wordnet')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/wordnet[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************
Traceback (most recent call last):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 84, in __load
    root = nltk.data.find(f"{self.subdir}/{zip_name}")
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93mwordnet[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('wordnet')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/wordnet.zip/wordnet/[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/stage_03_feature_engineering.py", line 180, in <module>
    pre.main(config_path=parsed_args.config)
  File "src/stage_03_feature_engineering.py", line 170, in main
    raise e
  File "src/stage_03_feature_engineering.py", line 165, in main
    df.loc[df['product'] == product, 'Rn'] = self.noun_score(data['answer_option'].values).values
  File "src/stage_03_feature_engineering.py", line 102, in noun_score
    noun_tag.append([lemmatizer.lemmatize(token.lemma_) for token in doc if
  File "src/stage_03_feature_engineering.py", line 102, in <listcomp>
    noun_tag.append([lemmatizer.lemmatize(token.lemma_) for token in doc if
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\stem\wordnet.py", line 45, in lemmatize
    lemmas = wn._morphy(word, pos)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 121, in __getattr__
    self.__load()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 86, in __load
    raise e
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 81, in __load
    root = nltk.data.find(f"{self.subdir}/{self.__name}")
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93mwordnet[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('wordnet')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/wordnet[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************

[2022-03-27 12:51:05,739: INFO: stage_01_get_data]: 
********************
[2022-03-27 12:51:05,786: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data started <<<<<
[2022-03-27 12:51:05,786: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-27 12:51:05,801: INFO: common]: yaml file: configs/credentials.yaml loaded successfully
[2022-03-27 12:51:05,801: INFO: common]: created directory at: data
[2022-03-27 12:51:05,801: INFO: common]: created directory at: data\extracted_data
[2022-03-27 12:51:05,801: INFO: stage_01_get_data]: verifying the credentials
[2022-03-27 12:51:15,648: INFO: stage_01_get_data]: Started downloading the data
[2022-03-27 12:51:20,261: INFO: stage_01_get_data]: Successfully saved the data in data\extracted_data\products.csv
[2022-03-27 12:51:20,277: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data completed!<<<<<

[2022-03-27 12:54:22,916: INFO: stage_02_preprocessing]: 
********************
[2022-03-27 12:54:22,916: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Preprocessing Data started <<<<<
[2022-03-27 12:54:22,932: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-27 12:54:22,932: INFO: common]: created directory at: data\processed_data
[2022-03-27 12:54:23,010: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection started
[2022-03-27 12:54:47,332: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection Ended
[2022-03-27 12:54:47,332: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Started
[2022-03-27 12:54:47,751: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Ended
[2022-03-27 12:54:47,751: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Started
[2022-03-27 12:54:48,237: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Ended
[2022-03-27 12:54:48,237: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Started
[2022-03-27 12:54:49,241: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Ended
[2022-03-27 12:54:49,429: INFO: stage_02_preprocessing]: Completed all the Stages in preprocessing data was stored in data\processed_data\processed.csv
[2022-03-27 12:54:49,429: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Preprocessing Data completed!<<<<<

[2022-03-27 12:57:03,024: INFO: stage_03_feature_engineering]: 
********************
[2022-03-27 12:57:03,024: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering started <<<<<
[2022-03-27 12:57:03,024: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-27 12:57:03,024: INFO: common]: created directory at: data\feature_processed
[2022-03-27 12:57:03,042: INFO: stage_03_feature_engineering]:  Added New Columns {Noun Strength,Review Polarity,Review Subjectivity,Review Complexity,Service Tagger,Compound Score}
[2022-03-27 12:57:17,945: ERROR: stage_03_feature_engineering]: 
**********************************************************************
  Resource [93mwordnet[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('wordnet')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/wordnet[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************
Traceback (most recent call last):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 84, in __load
    root = nltk.data.find(f"{self.subdir}/{zip_name}")
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93mwordnet[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('wordnet')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/wordnet.zip/wordnet/[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/stage_03_feature_engineering.py", line 165, in main
    df.loc[df['product'] == product, 'Rn'] = self.noun_score(data['answer_option'].values).values
  File "src/stage_03_feature_engineering.py", line 102, in noun_score
    noun_tag.append([lemmatizer.lemmatize(token.lemma_) for token in doc if
  File "src/stage_03_feature_engineering.py", line 102, in <listcomp>
    noun_tag.append([lemmatizer.lemmatize(token.lemma_) for token in doc if
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\stem\wordnet.py", line 45, in lemmatize
    lemmas = wn._morphy(word, pos)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 121, in __getattr__
    self.__load()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 86, in __load
    raise e
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 81, in __load
    root = nltk.data.find(f"{self.subdir}/{self.__name}")
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93mwordnet[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('wordnet')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/wordnet[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************

[2022-03-27 12:57:17,977: ERROR: stage_03_feature_engineering]: 
**********************************************************************
  Resource [93mwordnet[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('wordnet')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/wordnet[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************
Traceback (most recent call last):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 84, in __load
    root = nltk.data.find(f"{self.subdir}/{zip_name}")
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93mwordnet[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('wordnet')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/wordnet.zip/wordnet/[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/stage_03_feature_engineering.py", line 180, in <module>
    pre.main(config_path=parsed_args.config)
  File "src/stage_03_feature_engineering.py", line 170, in main
    raise e
  File "src/stage_03_feature_engineering.py", line 165, in main
    df.loc[df['product'] == product, 'Rn'] = self.noun_score(data['answer_option'].values).values
  File "src/stage_03_feature_engineering.py", line 102, in noun_score
    noun_tag.append([lemmatizer.lemmatize(token.lemma_) for token in doc if
  File "src/stage_03_feature_engineering.py", line 102, in <listcomp>
    noun_tag.append([lemmatizer.lemmatize(token.lemma_) for token in doc if
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\stem\wordnet.py", line 45, in lemmatize
    lemmas = wn._morphy(word, pos)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 121, in __getattr__
    self.__load()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 86, in __load
    raise e
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 81, in __load
    root = nltk.data.find(f"{self.subdir}/{self.__name}")
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93mwordnet[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('wordnet')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/wordnet[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************

[2022-03-27 13:00:45,104: INFO: stage_03_feature_engineering]: 
********************
[2022-03-27 13:00:45,104: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering started <<<<<
[2022-03-27 13:00:45,104: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-27 13:00:45,104: INFO: common]: created directory at: data\feature_processed
[2022-03-27 13:00:45,119: INFO: stage_03_feature_engineering]:  Added New Columns {Noun Strength,Review Polarity,Review Subjectivity,Review Complexity,Service Tagger,Compound Score}
[2022-03-27 13:00:59,487: ERROR: stage_03_feature_engineering]: 
**********************************************************************
  Resource [93momw-1.4[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('omw-1.4')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/omw-1.4[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************
Traceback (most recent call last):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 84, in __load
    root = nltk.data.find(f"{self.subdir}/{zip_name}")
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93momw-1.4[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('omw-1.4')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/omw-1.4.zip/omw-1.4/[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/stage_03_feature_engineering.py", line 167, in main
    df.loc[df['product'] == product, 'Rn'] = self.noun_score(data['answer_option'].values).values
  File "src/stage_03_feature_engineering.py", line 104, in noun_score
    noun_tag.append([lemmatizer.lemmatize(token.lemma_) for token in doc if
  File "src/stage_03_feature_engineering.py", line 104, in <listcomp>
    noun_tag.append([lemmatizer.lemmatize(token.lemma_) for token in doc if
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\stem\wordnet.py", line 45, in lemmatize
    lemmas = wn._morphy(word, pos)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 121, in __getattr__
    self.__load()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 89, in __load
    corpus = self.__reader_cls(root, *self.__args, **self.__kwargs)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\reader\wordnet.py", line 1176, in __init__
    self.provenances = self.omw_prov()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\reader\wordnet.py", line 1285, in omw_prov
    fileids = self._omw_reader.fileids()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 121, in __getattr__
    self.__load()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 86, in __load
    raise e
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 81, in __load
    root = nltk.data.find(f"{self.subdir}/{self.__name}")
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93momw-1.4[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('omw-1.4')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/omw-1.4[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************

[2022-03-27 13:00:59,491: ERROR: stage_03_feature_engineering]: 
**********************************************************************
  Resource [93momw-1.4[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('omw-1.4')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/omw-1.4[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************
Traceback (most recent call last):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 84, in __load
    root = nltk.data.find(f"{self.subdir}/{zip_name}")
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93momw-1.4[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('omw-1.4')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/omw-1.4.zip/omw-1.4/[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/stage_03_feature_engineering.py", line 182, in <module>
    pre.main(config_path=parsed_args.config)
  File "src/stage_03_feature_engineering.py", line 172, in main
    raise e
  File "src/stage_03_feature_engineering.py", line 167, in main
    df.loc[df['product'] == product, 'Rn'] = self.noun_score(data['answer_option'].values).values
  File "src/stage_03_feature_engineering.py", line 104, in noun_score
    noun_tag.append([lemmatizer.lemmatize(token.lemma_) for token in doc if
  File "src/stage_03_feature_engineering.py", line 104, in <listcomp>
    noun_tag.append([lemmatizer.lemmatize(token.lemma_) for token in doc if
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\stem\wordnet.py", line 45, in lemmatize
    lemmas = wn._morphy(word, pos)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 121, in __getattr__
    self.__load()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 89, in __load
    corpus = self.__reader_cls(root, *self.__args, **self.__kwargs)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\reader\wordnet.py", line 1176, in __init__
    self.provenances = self.omw_prov()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\reader\wordnet.py", line 1285, in omw_prov
    fileids = self._omw_reader.fileids()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 121, in __getattr__
    self.__load()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 86, in __load
    raise e
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\corpus\util.py", line 81, in __load
    root = nltk.data.find(f"{self.subdir}/{self.__name}")
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\nltk\data.py", line 583, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93momw-1.4[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('omw-1.4')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mcorpora/omw-1.4[0m

  Searched in:
    - 'C:\\Users\\Dheeraj kumar/nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\share\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\anaconda3\\envs\\sentiment-analysis\\lib\\nltk_data'
    - 'C:\\Users\\Dheeraj kumar\\AppData\\Roaming\\nltk_data'
    - 'C:\\nltk_data'
    - 'D:\\nltk_data'
    - 'E:\\nltk_data'
**********************************************************************

[2022-03-27 13:04:22,773: INFO: stage_03_feature_engineering]: 
********************
[2022-03-27 13:04:22,773: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering started <<<<<
[2022-03-27 13:04:22,782: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-27 13:04:22,782: INFO: common]: created directory at: data\feature_processed
[2022-03-27 13:04:22,782: INFO: stage_03_feature_engineering]:  Added New Columns {Noun Strength,Review Polarity,Review Subjectivity,Review Complexity,Service Tagger,Compound Score}
[2022-03-27 13:05:50,210: INFO: stage_03_feature_engineering]: Successfully saved the file in data\feature_processed\feature.csv
[2022-03-27 13:05:50,210: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering completed!<<<<<

[2022-03-27 13:06:51,523: INFO: stage_03_feature_engineering]: 
********************
[2022-03-27 13:06:51,523: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering started <<<<<
[2022-03-27 13:06:51,523: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-03-27 13:06:51,523: INFO: common]: created directory at: data\feature_processed
[2022-03-27 13:06:51,538: INFO: stage_03_feature_engineering]:  Added New Columns {Noun Strength,Review Polarity,Review Subjectivity,Review Complexity,Service Tagger,Compound Score}
[2022-03-27 13:08:20,440: INFO: stage_03_feature_engineering]: Successfully saved the file in data\feature_processed\feature.csv
[2022-03-27 13:08:20,440: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering completed!<<<<<

[2022-04-01 19:27:01,759: INFO: stage_01_get_data]: 
********************
[2022-04-01 19:27:01,784: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data started <<<<<
[2022-04-01 19:27:01,790: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-01 19:27:01,792: INFO: common]: yaml file: configs/credentials.yaml loaded successfully
[2022-04-01 19:27:01,792: INFO: common]: created directory at: data
[2022-04-01 19:27:01,792: INFO: common]: created directory at: data\extracted_data
[2022-04-01 19:27:01,792: INFO: stage_01_get_data]: verifying the credentials
[2022-04-01 19:27:06,305: INFO: stage_01_get_data]: Started downloading the data
[2022-04-01 19:27:06,978: INFO: stage_01_get_data]: Successfully saved the data in data\extracted_data\products.csv
[2022-04-01 19:27:06,981: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data completed!<<<<<

[2022-04-01 19:30:48,119: INFO: stage_02_preprocessing]: 
********************
[2022-04-01 19:30:48,119: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Preprocessing Data started <<<<<
[2022-04-01 19:30:48,123: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-01 19:30:48,123: INFO: common]: created directory at: data\processed_data
[2022-04-01 19:30:48,316: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection started
[2022-04-01 19:31:12,789: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection Ended
[2022-04-01 19:31:12,790: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Started
[2022-04-01 19:31:13,235: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Ended
[2022-04-01 19:31:13,235: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Started
[2022-04-01 19:31:13,757: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Ended
[2022-04-01 19:31:13,757: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Started
[2022-04-01 19:31:14,563: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Ended
[2022-04-01 19:31:14,853: INFO: stage_02_preprocessing]: Completed all the Stages in preprocessing data was stored in data\processed_data\processed.csv
[2022-04-01 19:31:14,853: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Preprocessing Data completed!<<<<<

[2022-04-01 19:32:46,766: INFO: stage_02_preprocessing]: 
********************
[2022-04-01 19:32:46,766: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 processing Data started <<<<<
[2022-04-01 19:32:46,769: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-01 19:32:46,770: INFO: common]: created directory at: data\processed_data
[2022-04-01 19:32:46,779: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection started
[2022-04-01 19:33:10,225: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection Ended
[2022-04-01 19:33:10,225: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Started
[2022-04-01 19:33:10,612: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Ended
[2022-04-01 19:33:10,612: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Started
[2022-04-01 19:33:11,098: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Ended
[2022-04-01 19:33:11,099: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Started
[2022-04-01 19:33:11,916: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Ended
[2022-04-01 19:33:11,927: INFO: stage_02_preprocessing]: Completed all the Stages in preprocessing data was stored in data\processed_data\processed.csv
[2022-04-01 19:33:11,927: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 processing Data completed!<<<<<

[2022-04-01 19:36:35,684: INFO: stage_03_feature_engineering]: 
********************
[2022-04-01 19:36:35,685: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering started <<<<<
[2022-04-01 19:36:35,688: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-01 19:36:35,688: INFO: common]: created directory at: data\feature_processed
[2022-04-01 19:36:35,699: INFO: stage_03_feature_engineering]:  Added New Columns {Noun Strength,Review Polarity,Review Subjectivity,Review Complexity,Service Tagger,Compound Score}
[2022-04-01 19:38:09,010: INFO: stage_03_feature_engineering]: Successfully saved the file in data\feature_processed\feature.csv
[2022-04-01 19:38:09,010: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering completed!<<<<<

[2022-04-01 19:38:22,897: INFO: stage_01_get_data]: 
********************
[2022-04-01 19:38:22,897: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data from S3Bucket started <<<<<
[2022-04-01 19:38:22,900: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-01 19:38:22,902: INFO: common]: yaml file: configs/credentials.yaml loaded successfully
[2022-04-01 19:38:22,902: INFO: common]: created directory at: data
[2022-04-01 19:38:22,902: INFO: common]: created directory at: data\extracted_data
[2022-04-01 19:38:22,902: INFO: stage_01_get_data]: verifying the credentials
[2022-04-01 19:38:27,630: INFO: stage_01_get_data]: Started downloading the data
[2022-04-01 19:38:28,285: INFO: stage_01_get_data]: Successfully saved the data in data\extracted_data\products.csv
[2022-04-01 19:38:28,287: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data from S3Bucket completed!<<<<<

[2022-04-01 19:39:34,266: INFO: stage_03_feature_engineering]: 
********************
[2022-04-01 19:39:34,266: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering Step started <<<<<
[2022-04-01 19:39:34,269: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-01 19:39:34,270: INFO: common]: created directory at: data\feature_processed
[2022-04-01 19:39:34,279: INFO: stage_03_feature_engineering]:  Added New Columns {Noun Strength,Review Polarity,Review Subjectivity,Review Complexity,Service Tagger,Compound Score}
[2022-04-01 19:41:02,369: INFO: stage_03_feature_engineering]: Successfully saved the file in data\feature_processed\feature.csv
[2022-04-01 19:41:02,370: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering Step completed!<<<<<

[2022-04-01 19:41:18,661: INFO: stage_02_preprocessing]: 
********************
[2022-04-01 19:41:18,661: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 PreProcessing Data started <<<<<
[2022-04-01 19:41:18,666: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-01 19:41:18,666: INFO: common]: created directory at: data\processed_data
[2022-04-01 19:41:18,676: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection started
[2022-04-01 19:41:41,285: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection Ended
[2022-04-01 19:41:41,286: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Started
[2022-04-01 19:41:41,685: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Ended
[2022-04-01 19:41:41,685: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Started
[2022-04-01 19:41:42,200: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Ended
[2022-04-01 19:41:42,200: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Started
[2022-04-01 19:41:43,006: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Ended
[2022-04-01 19:41:43,016: INFO: stage_02_preprocessing]: Completed all the Stages in preprocessing data was stored in data\processed_data\processed.csv
[2022-04-01 19:41:43,016: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 PreProcessing Data completed!<<<<<

[2022-04-01 19:41:46,032: INFO: stage_01_get_data]: 
********************
[2022-04-01 19:41:46,032: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data from S3 Bucket started <<<<<
[2022-04-01 19:41:46,035: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-01 19:41:46,037: INFO: common]: yaml file: configs/credentials.yaml loaded successfully
[2022-04-01 19:41:46,037: INFO: common]: created directory at: data
[2022-04-01 19:41:46,037: INFO: common]: created directory at: data\extracted_data
[2022-04-01 19:41:46,037: INFO: stage_01_get_data]: verifying the credentials
[2022-04-01 19:41:58,443: INFO: stage_01_get_data]: Started downloading the data
[2022-04-01 19:41:59,033: INFO: stage_01_get_data]: Successfully saved the data in data\extracted_data\products.csv
[2022-04-01 19:41:59,035: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data from S3 Bucket completed!<<<<<

[2022-04-01 21:03:46,433: INFO: stage_02_preprocessing]: 
********************
[2022-04-01 21:03:46,455: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Processing Data started <<<<<
[2022-04-01 21:03:46,472: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-01 21:03:46,472: INFO: common]: created directory at: data\processed_data
[2022-04-01 21:03:46,793: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection started
[2022-04-01 21:04:14,172: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection Ended
[2022-04-01 21:04:14,173: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Started
[2022-04-01 21:04:14,641: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Ended
[2022-04-01 21:04:14,641: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Started
[2022-04-01 21:04:15,498: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Ended
[2022-04-01 21:04:15,498: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Started
[2022-04-01 21:04:16,759: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Ended
[2022-04-01 21:04:16,943: INFO: stage_02_preprocessing]: Completed all the Stages in preprocessing data was stored in data\processed_data\processed.csv
[2022-04-01 21:04:16,943: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Processing Data completed!<<<<<

[2022-04-03 15:34:38,657: INFO: stage_03_feature_engineering]: 
********************
[2022-04-03 15:34:38,706: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering Step started <<<<<
[2022-04-03 15:34:38,774: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-03 15:34:38,775: INFO: common]: created directory at: data\feature_processed
[2022-04-03 15:34:38,781: INFO: stage_03_feature_engineering]:  Added New Columns {Noun Strength,Review Polarity,Review Subjectivity,Review Complexity,Service Tagger,Compound Score}
[2022-04-03 15:34:38,913: INFO: stage_03_feature_engineering]: Successfully saved the file in data\feature_processed\feature.csv
[2022-04-03 15:34:38,913: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering Step completed!<<<<<

[2022-04-03 15:38:55,082: INFO: stage_02_preprocessing]: 
********************
[2022-04-03 15:38:55,082: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Processing Data started <<<<<
[2022-04-03 15:38:55,085: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-03 15:38:55,086: INFO: common]: created directory at: data\processed_data
[2022-04-03 15:38:55,180: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection started
[2022-04-03 15:39:20,206: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection Ended
[2022-04-03 15:39:20,206: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Started
[2022-04-03 15:39:20,897: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Ended
[2022-04-03 15:39:20,897: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Started
[2022-04-03 15:39:21,479: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Ended
[2022-04-03 15:39:21,479: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Started
[2022-04-03 15:39:22,664: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Ended
[2022-04-03 15:39:23,030: INFO: stage_02_preprocessing]: Completed all the Stages in preprocessing data was stored in data\processed_data\processed.csv
[2022-04-03 15:39:23,030: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Processing Data completed!<<<<<

[2022-04-03 15:43:27,699: INFO: stage_03_feature_engineering]: 
********************
[2022-04-03 15:43:27,699: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering Step started <<<<<
[2022-04-03 15:43:27,704: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-03 15:43:27,704: INFO: common]: created directory at: data\feature_processed
[2022-04-03 15:43:27,712: INFO: stage_03_feature_engineering]:  Added New Columns {Noun Strength,Review Polarity,Review Subjectivity,Review Complexity,Service Tagger,Compound Score}
[2022-04-03 15:44:59,234: INFO: stage_03_feature_engineering]: Successfully saved the file in data\feature_processed\feature.csv
[2022-04-03 15:44:59,235: INFO: stage_03_feature_engineering]: >>>>>> stage Stage 03 Feature Engineering Step completed!<<<<<<

[2022-04-04 09:24:19,469: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 10:22:17,700: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 10:35:36,482: INFO: common]: yaml file: params.yaml loaded successfully
[2022-04-04 10:37:52,302: INFO: common]: yaml file: configs/credentials.yaml loaded successfully
[2022-04-04 10:37:52,308: INFO: common]: yaml file: params.yaml loaded successfully
[2022-04-04 10:39:52,236: INFO: common]: yaml file: configs/credentials.yaml loaded successfully
[2022-04-04 10:39:52,239: INFO: common]: yaml file: params.yaml loaded successfully
[2022-04-04 11:16:15,758: INFO: stage_04_model_training]: 
********************
[2022-04-04 11:16:15,758: INFO: stage_04_model_training]: >>>>> stage Stage 04 Model Training started <<<<<
[2022-04-04 11:16:15,758: INFO: stage_04_model_training]: Started 4th Stage
[2022-04-04 11:16:15,763: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 11:16:15,764: INFO: common]: yaml file: params.yaml loaded successfully
[2022-04-04 11:16:15,773: INFO: stage_04_model_training]: The unique products are ['Accucheck' 'shampoo' 'Becadexamin' 'Neurobion' 'Shelcal' 'Supradyn'
 'SevenseascodLiverOil' 'Evion']
[2022-04-04 11:16:17,231: INFO: stage_04_model_training]: Completed Building Training Data
[2022-04-04 11:16:22,765: INFO: stage_04_model_training]: Successfully saved parameters file in report/params.json
[2022-04-04 11:16:23,360: ERROR: stage_04_model_training]: Object of type ndarray is not JSON serializable
Traceback (most recent call last):
  File "src/stage_04_model_training.py", line 83, in main
    json.dump(scores, f, indent=4)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\json\__init__.py", line 179, in dump
    for chunk in iterable:
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\json\encoder.py", line 431, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\json\encoder.py", line 405, in _iterencode_dict
    yield from chunks
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\json\encoder.py", line 438, in _iterencode
    o = _default(o)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\json\encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type ndarray is not JSON serializable
[2022-04-04 11:19:42,380: INFO: stage_04_model_training]: 
********************
[2022-04-04 11:19:42,381: INFO: stage_04_model_training]: >>>>> stage Stage 04 Model Training started <<<<<
[2022-04-04 11:19:42,381: INFO: stage_04_model_training]: Started 4th Stage
[2022-04-04 11:19:42,385: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 11:19:42,387: INFO: common]: yaml file: params.yaml loaded successfully
[2022-04-04 11:19:42,395: INFO: stage_04_model_training]: The unique products are ['Accucheck' 'shampoo' 'Becadexamin' 'Neurobion' 'Shelcal' 'Supradyn'
 'SevenseascodLiverOil' 'Evion']
[2022-04-04 11:19:43,830: INFO: stage_04_model_training]: Completed Building Training Data
[2022-04-04 11:19:48,942: INFO: stage_04_model_training]: Successfully saved parameters file in report/params.json
[2022-04-04 11:19:49,437: INFO: stage_04_model_training]: Successfully saved scores file in  report/scores.json
[2022-04-04 11:19:49,655: INFO: stage_04_model_training]: Successfully saved model file in models\random_forest.joblib
[2022-04-04 11:19:49,668: INFO: stage_04_model_training]: >>>>>> stage Stage 04 Model Training completed!<<<<<<

[2022-04-04 11:21:41,061: INFO: stage_04_model_training]: 
********************
[2022-04-04 11:21:41,061: INFO: stage_04_model_training]: >>>>> stage Stage 04 Model Training started <<<<<
[2022-04-04 11:21:41,062: INFO: stage_04_model_training]: Started 4th Stage
[2022-04-04 11:21:41,066: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 11:21:41,068: INFO: common]: yaml file: params.yaml loaded successfully
[2022-04-04 11:21:41,076: INFO: stage_04_model_training]: The unique products are ['Accucheck' 'shampoo' 'Becadexamin' 'Neurobion' 'Shelcal' 'Supradyn'
 'SevenseascodLiverOil' 'Evion']
[2022-04-04 11:21:42,541: INFO: stage_04_model_training]: Completed Building Training Data
[2022-04-04 11:21:47,640: INFO: stage_04_model_training]: Successfully saved parameters file in report/params.json
[2022-04-04 11:21:48,109: INFO: stage_04_model_training]: Successfully saved scores file in  report/scores.json
[2022-04-04 11:21:48,268: INFO: stage_04_model_training]: Successfully saved model file in models\random_forest.joblib
[2022-04-04 11:21:48,281: INFO: stage_04_model_training]: >>>>>> stage Stage 04 Model Training completed!<<<<<<

[2022-04-04 13:50:29,455: INFO: stage_04_model_training]: 
********************
[2022-04-04 13:50:29,455: INFO: stage_04_model_training]: >>>>> stage Stage 04 Model Training started <<<<<
[2022-04-04 13:50:29,455: INFO: stage_04_model_training]: Started 4th Stage
[2022-04-04 13:50:29,462: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 13:50:29,464: INFO: common]: yaml file: params.yaml loaded successfully
[2022-04-04 13:50:29,476: INFO: stage_04_model_training]: The unique products are ['Accucheck' 'shampoo' 'Becadexamin' 'Neurobion' 'Shelcal' 'Supradyn'
 'SevenseascodLiverOil' 'Evion']
[2022-04-04 13:50:31,129: INFO: stage_04_model_training]: Completed Building Training Data
[2022-04-04 13:50:36,331: INFO: stage_04_model_training]: Successfully saved parameters file in report/params.json
[2022-04-04 13:50:36,819: INFO: stage_04_model_training]: Successfully saved scores file in  report/scores.json
[2022-04-04 13:50:36,923: INFO: stage_04_model_training]: Successfully saved model file in models\random_forest.joblib
[2022-04-04 13:50:36,931: ERROR: stage_04_model_training]: name 'score_giver' is not defined
Traceback (most recent call last):
  File "src/stage_04_model_training.py", line 108, in main
    score = score_giver(C, D)
NameError: name 'score_giver' is not defined
[2022-04-04 13:51:06,341: INFO: stage_04_model_training]: 
********************
[2022-04-04 13:51:06,342: INFO: stage_04_model_training]: >>>>> stage Stage 04 Model Training started <<<<<
[2022-04-04 13:51:06,342: INFO: stage_04_model_training]: Started 4th Stage
[2022-04-04 13:51:06,346: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 13:51:06,348: INFO: common]: yaml file: params.yaml loaded successfully
[2022-04-04 13:51:06,360: INFO: stage_04_model_training]: The unique products are ['Accucheck' 'shampoo' 'Becadexamin' 'Neurobion' 'Shelcal' 'Supradyn'
 'SevenseascodLiverOil' 'Evion']
[2022-04-04 13:51:07,898: INFO: stage_04_model_training]: Completed Building Training Data
[2022-04-04 13:51:13,106: INFO: stage_04_model_training]: Successfully saved parameters file in report/params.json
[2022-04-04 13:51:13,600: INFO: stage_04_model_training]: Successfully saved scores file in  report/scores.json
[2022-04-04 13:51:13,826: INFO: stage_04_model_training]: Successfully saved model file in models\random_forest.joblib
[2022-04-04 13:55:07,528: INFO: stage_04_model_training]: Successfully saved the rank file in data\feature_processed\rank.csv
[2022-04-04 13:55:07,556: INFO: stage_04_model_training]: >>>>>> stage Stage 04 Model Training completed!<<<<<<

[2022-04-04 18:12:45,689: INFO: stage_03_feature_engineering]: 
********************
[2022-04-04 18:12:45,705: INFO: stage_03_feature_engineering]: >>>>> stage Stage 03 Feature Engineering Step started <<<<<
[2022-04-04 18:12:45,705: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 18:12:45,705: INFO: common]: created directory at: data\feature_processed
[2022-04-04 18:12:45,721: INFO: stage_03_feature_engineering]:  Added New Columns {Noun Strength,Review Polarity,Review Subjectivity,Review Complexity,Service Tagger,Compound Score}
[2022-04-04 18:14:27,023: INFO: stage_03_feature_engineering]: Successfully saved the file in data\feature_processed\feature.csv
[2022-04-04 18:14:27,023: INFO: stage_03_feature_engineering]: >>>>>> stage Stage 03 Feature Engineering Step completed!<<<<<<

[2022-04-04 18:14:32,899: INFO: stage_04_model_training]: 
********************
[2022-04-04 18:14:32,899: INFO: stage_04_model_training]: >>>>> stage Stage 04 Model Training started <<<<<
[2022-04-04 18:14:32,899: INFO: stage_04_model_training]: Started 4th Stage
[2022-04-04 18:14:32,899: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 18:14:32,899: INFO: common]: yaml file: params.yaml loaded successfully
[2022-04-04 18:14:32,914: INFO: stage_04_model_training]: The unique products are ['Accucheck' 'shampoo' 'Becadexamin' 'Neurobion' 'Shelcal' 'Supradyn'
 'SevenseascodLiverOil' 'Evion']
[2022-04-04 18:14:34,617: INFO: stage_04_model_training]: Completed Building Training Data
[2022-04-04 18:14:40,121: INFO: stage_04_model_training]: Successfully saved parameters file in report/params.json
[2022-04-04 18:14:40,663: INFO: stage_04_model_training]: Successfully saved scores file in  report/scores.json
[2022-04-04 18:14:40,826: INFO: stage_04_model_training]: Successfully saved model file in models\random_forest.joblib
[2022-04-04 18:18:39,068: INFO: stage_04_model_training]: Successfully saved the rank file in data\feature_processed\rank.csv
[2022-04-04 18:18:39,084: INFO: stage_04_model_training]: >>>>>> stage Stage 04 Model Training completed!<<<<<<

[2022-04-04 18:19:50,853: INFO: stage_02_preprocessing]: 
********************
[2022-04-04 18:19:50,853: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Processing Data started <<<<<
[2022-04-04 18:19:50,853: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 18:19:50,853: INFO: common]: created directory at: data\processed_data
[2022-04-04 18:19:50,884: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection started
[2022-04-04 18:20:22,991: INFO: stage_02_preprocessing]: Step-1 ==> Language Detection Ended
[2022-04-04 18:20:22,991: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Started
[2022-04-04 18:20:23,555: INFO: stage_02_preprocessing]: Step-2 ==> Gibberish Detection Ended
[2022-04-04 18:20:23,555: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Started
[2022-04-04 18:20:24,105: INFO: stage_02_preprocessing]: Step-3 ==> Profanity Detection Ended
[2022-04-04 18:20:24,105: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Started
[2022-04-04 18:20:25,039: INFO: stage_02_preprocessing]: Step-4 ==> Company Tag Removal Ended
[2022-04-04 18:20:25,071: INFO: stage_02_preprocessing]: Completed all the Stages in preprocessing data was stored in data\processed_data\processed.csv
[2022-04-04 18:20:25,071: INFO: stage_02_preprocessing]: >>>>> stage Stage 02 Processing Data completed!<<<<<

[2022-04-04 18:20:28,475: INFO: stage_04_model_training]: 
********************
[2022-04-04 18:20:28,475: INFO: stage_04_model_training]: >>>>> stage Stage 04 Model Training started <<<<<
[2022-04-04 18:20:28,475: INFO: stage_04_model_training]: Started 4th Stage
[2022-04-04 18:20:28,475: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 18:20:28,475: INFO: common]: yaml file: params.yaml loaded successfully
[2022-04-04 18:20:28,491: INFO: stage_04_model_training]: The unique products are ['Accucheck' 'shampoo' 'Becadexamin' 'Neurobion' 'Shelcal' 'Supradyn'
 'SevenseascodLiverOil' 'Evion']
[2022-04-04 18:20:30,305: INFO: stage_04_model_training]: Completed Building Training Data
[2022-04-04 18:20:36,255: INFO: stage_04_model_training]: Successfully saved parameters file in report/params.json
[2022-04-04 18:20:36,898: INFO: stage_04_model_training]: Successfully saved scores file in  report/scores.json
[2022-04-04 18:20:37,023: INFO: stage_04_model_training]: Successfully saved model file in models\random_forest.joblib
[2022-04-04 18:24:48,635: INFO: stage_04_model_training]: Successfully saved the rank file in data\feature_processed\rank.csv
[2022-04-04 18:24:48,666: INFO: stage_04_model_training]: >>>>>> stage Stage 04 Model Training completed!<<<<<<

[2022-04-04 18:25:36,013: INFO: stage_01_get_data]: 
********************
[2022-04-04 18:25:36,013: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data from S3 Bucket started <<<<<
[2022-04-04 18:25:36,013: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 18:25:36,013: INFO: common]: yaml file: configs/credentials.yaml loaded successfully
[2022-04-04 18:25:36,013: INFO: common]: created directory at: data
[2022-04-04 18:25:36,013: INFO: common]: created directory at: data\extracted_data
[2022-04-04 18:25:36,013: INFO: stage_01_get_data]: verifying the credentials
[2022-04-04 18:26:03,024: INFO: stage_04_model_training]: 
********************
[2022-04-04 18:26:03,024: INFO: stage_04_model_training]: >>>>> stage Stage 04 Model Training started <<<<<
[2022-04-04 18:26:03,024: INFO: stage_04_model_training]: Started 4th Stage
[2022-04-04 18:26:03,040: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 18:26:03,040: INFO: common]: yaml file: params.yaml loaded successfully
[2022-04-04 18:26:03,040: INFO: stage_04_model_training]: The unique products are ['Accucheck' 'shampoo' 'Becadexamin' 'Neurobion' 'Shelcal' 'Supradyn'
 'SevenseascodLiverOil' 'Evion']
[2022-04-04 18:26:04,854: INFO: stage_04_model_training]: Completed Building Training Data
[2022-04-04 18:26:10,862: INFO: stage_04_model_training]: Successfully saved parameters file in report/params.json
[2022-04-04 18:26:11,521: INFO: stage_04_model_training]: Successfully saved scores file in  report/scores.json
[2022-04-04 18:26:11,679: INFO: stage_04_model_training]: Successfully saved model file in models\random_forest.joblib
[2022-04-04 18:30:12,518: INFO: stage_04_model_training]: Successfully saved the rank file in data\feature_processed\rank.csv
[2022-04-04 18:30:12,533: INFO: stage_04_model_training]: >>>>>> stage Stage 04 Model Training completed!<<<<<<

[2022-04-04 18:30:15,153: INFO: stage_01_get_data]: 
********************
[2022-04-04 18:30:15,153: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data from S3 Bucket started <<<<<
[2022-04-04 18:30:15,158: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 18:30:15,158: INFO: common]: yaml file: configs/credentials.yaml loaded successfully
[2022-04-04 18:30:15,158: INFO: common]: created directory at: data
[2022-04-04 18:30:15,158: INFO: common]: created directory at: data\extracted_data
[2022-04-04 18:30:15,158: INFO: stage_01_get_data]: verifying the credentials
[2022-04-04 18:30:25,000: ERROR: stage_01_get_data]: Could not connect to the endpoint URL: "https://dheeraj-2.s3.amazonaws.com/train.csv"
Traceback (most recent call last):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\util\connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\socket.py", line 918, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno 11001] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\httpsession.py", line 405, in send
    urllib_response = conn.urlopen(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connectionpool.py", line 785, in urlopen
    retries = retries.increment(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\util\retry.py", line 525, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\packages\six.py", line 770, in reraise
    raise value
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connectionpool.py", line 386, in _make_request
    self._validate_conn(conn)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connectionpool.py", line 1040, in _validate_conn
    conn.connect()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connection.py", line 358, in connect
    conn = self._new_conn()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <botocore.awsrequest.AWSHTTPSConnection object at 0x0000019055743700>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/stage_01_get_data.py", line 43, in main
    csv_obj = client.get_object(Bucket=bucketname, Key=filename)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\client.py", line 395, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\client.py", line 711, in _make_api_call
    http, parsed_response = self._make_request(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\client.py", line 731, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\endpoint.py", line 107, in make_request
    return self._send_request(request_dict, operation_model)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\endpoint.py", line 183, in _send_request
    while self._needs_retry(attempts, operation_model, request_dict,
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\endpoint.py", line 305, in _needs_retry
    responses = self._event_emitter.emit(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\hooks.py", line 357, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\hooks.py", line 228, in emit
    return self._emit(event_name, kwargs)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\hooks.py", line 211, in _emit
    response = handler(**kwargs)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\retryhandler.py", line 192, in __call__
    if self._checker(**checker_kwargs):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\retryhandler.py", line 265, in __call__
    should_retry = self._should_retry(attempt_number, response,
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\retryhandler.py", line 292, in _should_retry
    return self._checker(attempt_number, response, caught_exception)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\retryhandler.py", line 331, in __call__
    checker_response = checker(attempt_number, response,
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\retryhandler.py", line 231, in __call__
    return self._check_caught_exception(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\retryhandler.py", line 374, in _check_caught_exception
    raise caught_exception
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\endpoint.py", line 249, in _do_get_response
    http_response = self._send(request)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\endpoint.py", line 321, in _send
    return self.http_session.send(request)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\httpsession.py", line 434, in send
    raise EndpointConnectionError(endpoint_url=request.url, error=e)
botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL: "https://dheeraj-2.s3.amazonaws.com/train.csv"
[2022-04-04 18:30:25,376: ERROR: stage_01_get_data]: Could not connect to the endpoint URL: "https://dheeraj-2.s3.amazonaws.com/train.csv"
Traceback (most recent call last):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\util\connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\socket.py", line 918, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno 11001] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\httpsession.py", line 405, in send
    urllib_response = conn.urlopen(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connectionpool.py", line 785, in urlopen
    retries = retries.increment(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\util\retry.py", line 525, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\packages\six.py", line 770, in reraise
    raise value
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connectionpool.py", line 386, in _make_request
    self._validate_conn(conn)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connectionpool.py", line 1040, in _validate_conn
    conn.connect()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connection.py", line 358, in connect
    conn = self._new_conn()
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <botocore.awsrequest.AWSHTTPSConnection object at 0x0000019055743700>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/stage_01_get_data.py", line 62, in <module>
    main(config_path=parsed_args.config,creditionals_path=parsed_args.credentials)
  File "src/stage_01_get_data.py", line 52, in main
    raise e
  File "src/stage_01_get_data.py", line 43, in main
    csv_obj = client.get_object(Bucket=bucketname, Key=filename)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\client.py", line 395, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\client.py", line 711, in _make_api_call
    http, parsed_response = self._make_request(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\client.py", line 731, in _make_request
    return self._endpoint.make_request(operation_model, request_dict)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\endpoint.py", line 107, in make_request
    return self._send_request(request_dict, operation_model)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\endpoint.py", line 183, in _send_request
    while self._needs_retry(attempts, operation_model, request_dict,
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\endpoint.py", line 305, in _needs_retry
    responses = self._event_emitter.emit(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\hooks.py", line 357, in emit
    return self._emitter.emit(aliased_event_name, **kwargs)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\hooks.py", line 228, in emit
    return self._emit(event_name, kwargs)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\hooks.py", line 211, in _emit
    response = handler(**kwargs)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\retryhandler.py", line 192, in __call__
    if self._checker(**checker_kwargs):
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\retryhandler.py", line 265, in __call__
    should_retry = self._should_retry(attempt_number, response,
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\retryhandler.py", line 292, in _should_retry
    return self._checker(attempt_number, response, caught_exception)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\retryhandler.py", line 331, in __call__
    checker_response = checker(attempt_number, response,
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\retryhandler.py", line 231, in __call__
    return self._check_caught_exception(
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\retryhandler.py", line 374, in _check_caught_exception
    raise caught_exception
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\endpoint.py", line 249, in _do_get_response
    http_response = self._send(request)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\endpoint.py", line 321, in _send
    return self.http_session.send(request)
  File "C:\Users\Dheeraj kumar\anaconda3\envs\sentiment-analysis\lib\site-packages\botocore\httpsession.py", line 434, in send
    raise EndpointConnectionError(endpoint_url=request.url, error=e)
botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL: "https://dheeraj-2.s3.amazonaws.com/train.csv"
[2022-04-04 18:31:58,472: INFO: stage_01_get_data]: 
********************
[2022-04-04 18:31:58,472: INFO: stage_01_get_data]: >>>>> stage Stage 01 Extracting Data from S3 Bucket started <<<<<
[2022-04-04 18:31:58,488: INFO: common]: yaml file: configs/config.yaml loaded successfully
[2022-04-04 18:31:58,488: INFO: common]: yaml file: configs/credentials.yaml loaded successfully
[2022-04-04 18:31:58,488: INFO: common]: created directory at: data
[2022-04-04 18:31:58,488: INFO: common]: created directory at: data\extracted_data
[2022-04-04 18:31:58,488: INFO: stage_01_get_data]: verifying the credentials
